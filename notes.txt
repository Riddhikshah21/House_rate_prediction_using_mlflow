Machine Learning is everywhere
MLflow solves certain challenges that MLOps aims to address.

Problem 1: Some challenges of ML engineers: 
	- Reproducibility 
	- Traceability
	- Notes and spreadsheets are not scalable 
	
Solution 1: MLflow tracking: 
	- Stores data for all experiments
	- User interface
	- Programmatic querying 

Problem 2: After your model is trained:
	- Training and evaluation -> Model -> production env (batch predictions or real-time predictions) (batch predictions : airline engine model wants to update new prices twice a day on the website or real-time predictions: fraud detection of a transaction)
	- Once we decide our model - we need to handover model to software engineers with extensive explanations about what is required and they add additional layers of code. So, we have MLFlow models to take care of it.

Solution 2: MLflow models:
	- Format for packing models
	- Running predictions against existing dataset
	- Real-time serving 

Problem 3: Even after model is ready to put into production there are several challenges such as:
	- Model discovery - which model version is latest, which model is ready to put in production for business use
	- Model metadata management 
	- Traditionally requires explicit communications
	- Human error likely to happen

Solution 3: MLfLow registry: tagged, uploaded and discovered by others. 
	- Centralized model catalogue 
	- Models can be used/deployed directly from the registry
	- Can be used as a base for building automated tools

MLflow projects: another Mlflow component.

######################################################################################################################################

**Experimenting with Mlflow:**
	- to keep track of various runs we can perform and save the results for each run 
	- Load data -> preprocess data -> training the model -> evaluating the model 
	- You want to try different solutions to get satisfactory results, can lead to multiple iterations.

Experiment: table
Training execution (run): row in a table. 
Parameters: different items that when changes will affect the result of model.
Metrics: to determine model's performance. 

Mlflow UI - to visualize Mlflow experiments, metrics, parameters, models, etc. 

Artifacts: aside from parameters and metrics, we need to store files such as data that is transformed, model that can be a viable option in future, plots, etc.
When preparing for demo/presentations - need to manually maintain a directory structure that can be matched to experiments and runs. Everyone needs to adhere to the convention. 
Mlflow takes care of experiments and runs, it becomes easier to store files which saves a lot of time. Artifacts are files associated with runs. 

	- Exporting an artifact: stores the file in a temporary location on disk.
	- Log (export) it using Mlflow api and attach to the run

log_artifact will create a single artifact for the model but cannot visualize it 
log_artifacts can be used to save artifact for model, dataset and visualize data plots. 

######################################################################################################################################

**Collaborative scenario:**

Mlruns stores locally all the artifacts, metrics and parameters and start UI locally. But when you expand the team, you want to share results with each other.

Mlruns -> experiment id -> run id -> 1. artifacts 2. metrics 3. parameters.
For storing files in artifacts makes sense but maintaining two different files for metrics and parameters can be problematic when searching leading to slowness issues. 
	- Not scale well 
	- Bad when using shared network file system
	- Not great for single user with a substantial number of experiments. 
	
Adding storage location which can be shared network file system. 
	1. In python code add tracking url:
	Mlflow.set_tracking_url("file_path")
	2. Starting the UI:
	Mlflow ui --backend-store-url file_path 
	- Artifacts has folders with files but metrics and parameters are also stored for each run using its different id, but managing and searching in a lot of files can be problematic. 
	- Solution: use SQLite to store metrics and parameters value can help query in the db. It is a relational database 

After Using SQLite:
	- Artifact store: Artifacts are same as before
	- Backend store: Everything else in single sqlite file: mlruns.db

Using sqlite backend store: 
	1. In python code add tracking url:
	Mlflow.set_tracking_url("file_path_to_mlruns.db")
	2. Starting the UI:
	Mlflow ui --backend-store-url sqlite://file_path_to_mlruns.db
	
Shortcomings of using this approach:
	- SQLite will slow down if data increases.
	- Every user needs to have artifacts folder mounted on the same path 
	- Every user needs to run their own mlflow server
	
Solution :Complete MLflow deployment architecture.
	- Dedicated database - Replace backend store with a proper database like postgresql
	- Customized artifact store - some artifact store options are amazon s3, google cloud storage, azure blob storage
	- Separate server - replace the mlflow server with some company net name. 


######################################################################################################################################

**Mlflow models: Packaging and running models**
	1. Logging a model: supported flavor: eg. Scikit-learn -> mlflow.sklearn.log_model(trained_model, "model")
		○ Sklearn flavor:
		○ Logs a model in mlflow with python env, conda env, yml meta data file, pickle file of model, requiments.txt for tracking dependencies, etc. 
		○ It provides code for validation and prediction to be used for that model. 
	2. Customized model
		a. For logging model with customized preprocessing or any other steps to follow before predicting using the model trained on different data. 
		b. For flavor outside of mlflow: for custom model that cannot be used in sklearn flavor. 
			i. Pyfunc flavor
To create model with sklearn flavor by creating CustomModel class and adding methods such as predict, load_context that adds artifact of model into a dictionary that can be stored as sklearn flavor 





######################################################################################################################################